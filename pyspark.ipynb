{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cddb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+-------------+----------+\n",
      "|ID_produit|Nom_produit|Quantite_vendue|Prix_unitaire|Date_vente|\n",
      "+----------+-----------+---------------+-------------+----------+\n",
      "|         1|    Chemise|           10.0|         25.0|2022-01-05|\n",
      "|         2|   Pantalon|            8.0|         35.0|2022-01-06|\n",
      "|         3| Chaussures|           NULL|         50.0|2022-01-07|\n",
      "|         4|    Cravate|           12.0|         15.0|2022-01-08|\n",
      "|         5|       Robe|           15.0|         45.0|2022-01-09|\n",
      "+----------+-----------+---------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ID_produit: integer (nullable = true)\n",
      " |-- Nom_produit: string (nullable = true)\n",
      " |-- Quantite_vendue: double (nullable = true)\n",
      " |-- Prix_unitaire: double (nullable = true)\n",
      " |-- Date_vente: string (nullable = true)\n",
      "\n",
      "+-------+------------------+-----------+------------------+------------------+------------+\n",
      "|summary|        ID_produit|Nom_produit|   Quantite_vendue|     Prix_unitaire|  Date_vente|\n",
      "+-------+------------------+-----------+------------------+------------------+------------+\n",
      "|  count|              5266|       5255|              2594|              2697|        5266|\n",
      "|   mean|  50.7001519179643|       NULL| 10.81129529683887| 54.12290322580648|        NULL|\n",
      "| stddev|29.088180242092857|       NULL|5.5966885644726245|25.663199102855064|        NULL|\n",
      "|    min|                 1| Chaussures|               0.0|             10.04|  01/01/2024|\n",
      "|    max|               100|       Robe|              25.0|             99.93|invalid_date|\n",
      "+-------+------------------+-----------+------------------+------------------+------------+\n",
      "\n",
      "Nombre de lignes : 5266\n",
      "Nombre de colonnes : 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse de données ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Chargement des données\n",
    "df = spark.read.csv('./jeu_donnees_etl_5000_lignes.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Affichage des premières lignes\n",
    "df.show(5)  # équivalent de df.head() en Pandas\n",
    "\n",
    "# Informations générales (schéma et types)\n",
    "df.printSchema()\n",
    "\n",
    "# Statistiques descriptives\n",
    "df.describe().show()\n",
    "\n",
    "# Nombre de lignes et de colonnes\n",
    "print(f\"Nombre de lignes : {df.count()}\")\n",
    "print(f\"Nombre de colonnes : {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22e25b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avant suppression : 5266\n",
      "Nombre de lignes après suppression : 4965\n",
      "Nombre de doublons supprimés : 301\n"
     ]
    }
   ],
   "source": [
    "# Nombre de lignes avant suppression\n",
    "nb_lignes_avant = df.count()\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Nombre de lignes après suppression\n",
    "nb_lignes_apres = df.count()\n",
    "\n",
    "# Affichage du résultat\n",
    "print(f\"Nombre de lignes avant suppression : {nb_lignes_avant}\")\n",
    "print(f\"Nombre de lignes après suppression : {nb_lignes_apres}\")\n",
    "print(f\"Nombre de doublons supprimés : {nb_lignes_avant - nb_lignes_apres}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d67291a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de NaN dans la colonne 'Quantite_vendue' est : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Supprimer les lignes où 'Quantite_vendue' est manquante\n",
    "df = df.filter(col(\"Quantite_vendue\").isNotNull())\n",
    "\n",
    "# Convertir 'Quantite_vendue' en entier (si elle ne l'est pas déjà)\n",
    "df = df.withColumn(\"Quantite_vendue\", col(\"Quantite_vendue\").cast(\"int\"))\n",
    "\n",
    "# Vérifier le nombre de valeurs nulles dans la colonne 'Quantite_vendue'\n",
    "nb_nan = df.select(count(when(col(\"Quantite_vendue\").isNull(), True)).alias(\"nb_nan\")).collect()[0][\"nb_nan\"]\n",
    "print(f\"Le nombre de NaN dans la colonne 'Quantite_vendue' est : {nb_nan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c18bb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_produit: integer (nullable = true)\n",
      " |-- Nom_produit: string (nullable = true)\n",
      " |-- Quantite_vendue: integer (nullable = true)\n",
      " |-- Prix_unitaire: double (nullable = true)\n",
      " |-- Date_vente: string (nullable = true)\n",
      "\n",
      "+----------+-----------+---------------+-------------+------------+\n",
      "|ID_produit|Nom_produit|Quantite_vendue|Prix_unitaire|  Date_vente|\n",
      "+----------+-----------+---------------+-------------+------------+\n",
      "|        37|    Chemise|             10|         25.0|  2022-01-05|\n",
      "|        65|       Robe|             15|         45.0|  2022-01-09|\n",
      "|        64|    Cravate|             18|        87.41|  31/10/2024|\n",
      "|        56|       Robe|              8|        23.35|invalid_date|\n",
      "|        42|     Montre|              4|        40.91|invalid_date|\n",
      "+----------+-----------+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc164b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Fonction pour appliquer la winsorisation sur une colonne\n",
    "def winsorize_column(df, column_name):\n",
    "    # Calcul des quartiles\n",
    "    q1, q3 = df.approxQuantile(column_name, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    # Clipping des valeurs aberrantes\n",
    "    return df.withColumn(\n",
    "        column_name,\n",
    "        when(col(column_name) < lower, lower)\n",
    "        .when(col(column_name) > upper, upper)\n",
    "        .otherwise(col(column_name))\n",
    "    )\n",
    "\n",
    "# Appliquer sur les deux colonnes\n",
    "df = winsorize_column(df, \"Quantite_vendue\")\n",
    "df = winsorize_column(df, \"Prix_unitaire\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d240df81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doublons_restants : 6\n",
      "valeurs_manquantes : {'ID_produit': 0, 'Nom_produit': 10, 'Quantite_vendue': 0, 'Prix_unitaire': 1195, 'Date_vente': 0}\n",
      "quantite_negative : 0\n",
      "prix_negative : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Vérifier les doublons restants (lignes identiques)\n",
    "doublons_restants = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "# Compter les valeurs manquantes pour chaque colonne\n",
    "valeurs_manquantes = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Vérifier les valeurs négatives\n",
    "quantite_negative = df.filter(col(\"Quantite_vendue\") < 0).count()\n",
    "prix_negative = df.filter(col(\"Prix_unitaire\") < 0).count()\n",
    "\n",
    "# Résumé sous forme de dictionnaire\n",
    "validation = {\n",
    "    \"doublons_restants\": doublons_restants,\n",
    "    \"valeurs_manquantes\": valeurs_manquantes,\n",
    "    \"quantite_negative\": quantite_negative,\n",
    "    \"prix_negative\": prix_negative\n",
    "}\n",
    "\n",
    "# Afficher les résultats\n",
    "for cle, valeur in validation.items():\n",
    "    print(f\"{cle} : {valeur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cf4d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doublons_restants : 0\n",
      "valeurs_manquantes : {'ID_produit': 0, 'Nom_produit': 0, 'Quantite_vendue': 0, 'Prix_unitaire': 0, 'Date_vente': 0}\n",
      "quantite_negative : 0\n",
      "prix_negative : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# 1. Supprimer les lignes où 'Nom_produit' ou 'Prix_unitaire' sont nulles\n",
    "df = df.dropna(subset=['Nom_produit', 'Prix_unitaire'])\n",
    "\n",
    "# 2. Validation des données\n",
    "\n",
    "# Doublons restants\n",
    "doublons_restants = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "# Valeurs manquantes par colonne\n",
    "valeurs_manquantes = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Quantités négatives\n",
    "quantite_negative = df.filter(col(\"Quantite_vendue\") < 0).count()\n",
    "\n",
    "# Prix négatifs\n",
    "prix_negative = df.filter(col(\"Prix_unitaire\") < 0).count()\n",
    "\n",
    "# Résumé\n",
    "validation = {\n",
    "    \"doublons_restants\": doublons_restants,\n",
    "    \"valeurs_manquantes\": valeurs_manquantes,\n",
    "    \"quantite_negative\": quantite_negative,\n",
    "    \"prix_negative\": prix_negative\n",
    "}\n",
    "\n",
    "# Affichage\n",
    "for cle, valeur in validation.items():\n",
    "    print(f\"{cle} : {valeur}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed154031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "try:\n",
    "    df = df.withColumn(\"Valeur_totale\", col(\"Quantite_vendue\") * col(\"Prix_unitaire\"))\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors du calcul de la colonne 'Valeur_totale': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5eeacee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"Montant_total\", col(\"Quantite_vendue\") * col(\"Prix_unitaire\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "36a2ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min as spark_min, max as spark_max, col\n",
    "\n",
    "# Calcul des min et max avec alias différents\n",
    "stats = df.agg(\n",
    "    spark_min(\"Montant_total\").alias(\"min_montant\"),\n",
    "    spark_max(\"Montant_total\").alias(\"max_montant\")\n",
    ").collect()[0]\n",
    "\n",
    "montant_min = stats[\"min_montant\"]\n",
    "montant_max = stats[\"max_montant\"]\n",
    "\n",
    "# Création de la colonne normalisée\n",
    "df = df.withColumn(\n",
    "    \"Montant_total_normalise\",\n",
    "    (col(\"Montant_total\") - montant_min) / (montant_max - montant_min)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4433bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "df_agg = df.groupBy(\"Nom_produit\") \\\n",
    "           .agg(spark_sum(\"Valeur_totale\").alias(\"Vente_totale\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc0afb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitements effectués :\n",
      "['Suppression des doublons', 'Imputation des valeurs manquantes avec la médiane', 'Winsorisation des valeurs aberrantes', \"Ajout de la colonne 'Valeur_totale'\", 'Agrégation des ventes par produit']\n",
      "\n",
      "Validation finale :\n",
      "{'doublons_restants': 0, 'valeurs_manquantes': {'ID_produit': 0, 'Nom_produit': 0, 'Quantite_vendue': 0, 'Prix_unitaire': 0, 'Date_vente': 0, 'Valeur_totale': 0, 'Montant_total': 0, 'Montant_total_normalise': 0}, 'quantite_negative': 0, 'prix_negative': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Recalcul des validations finales\n",
    "\n",
    "doublons_restants = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "valeurs_manquantes = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "quantite_negative = df.filter(col(\"Quantite_vendue\") < 0).count()\n",
    "prix_negative = df.filter(col(\"Prix_unitaire\") < 0).count()\n",
    "\n",
    "documentation_resume = {\n",
    "    \"Traitements effectués\": [\n",
    "        \"Suppression des doublons\",\n",
    "        \"Imputation des valeurs manquantes avec la médiane\",\n",
    "        \"Winsorisation des valeurs aberrantes\",\n",
    "        \"Ajout de la colonne 'Valeur_totale'\",\n",
    "        \"Agrégation des ventes par produit\"\n",
    "    ],\n",
    "    \"Validation finale\": {\n",
    "        \"doublons_restants\": doublons_restants,\n",
    "        \"valeurs_manquantes\": valeurs_manquantes,\n",
    "        \"quantite_negative\": quantite_negative,\n",
    "        \"prix_negative\": prix_negative\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage lisible\n",
    "for cle, valeur in documentation_resume.items():\n",
    "    print(f\"{cle} :\")\n",
    "    print( valeur)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "016e23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+-------------+------------+------------------+------------------+-----------------------+\n",
      "|ID_produit|Nom_produit|Quantite_vendue|Prix_unitaire|  Date_vente|     Valeur_totale|     Montant_total|Montant_total_normalise|\n",
      "+----------+-----------+---------------+-------------+------------+------------------+------------------+-----------------------+\n",
      "|        37|    Chemise|           10.0|         25.0|  2022-01-05|             250.0|             250.0|    0.13262669828487156|\n",
      "|        65|       Robe|           15.0|         45.0|  2022-01-09|             675.0|             675.0|     0.3580920853691532|\n",
      "|        64|    Cravate|           18.0|        87.41|  31/10/2024|1573.3799999999999|1573.3799999999999|     0.8346887781898048|\n",
      "|        56|       Robe|            8.0|        23.35|invalid_date|             186.8|             186.8|    0.09909866895845602|\n",
      "|        42|     Montre|            4.0|        40.91|invalid_date|            163.64|            163.64|    0.08681213162934552|\n",
      "+----------+-----------+---------------+-------------+------------+------------------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)  # Affiche les 5 premières lignes sous forme tabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b0f4549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_produit: integer (nullable = true)\n",
      " |-- Nom_produit: string (nullable = true)\n",
      " |-- Quantite_vendue: double (nullable = true)\n",
      " |-- Prix_unitaire: double (nullable = true)\n",
      " |-- Date_vente: string (nullable = true)\n",
      " |-- Valeur_totale: double (nullable = true)\n",
      " |-- Montant_total: double (nullable = true)\n",
      " |-- Montant_total_normalise: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
